import numpy as np
import matplotlib.pyplot as plt
from load_data import load_data
from scipy.signal import argrelextrema
import sys
from sklearn import svm
from sklearn.ensemble import AdaBoostRegressor

from sklearn.linear_model import LogisticRegression
from sklearn import cross_validation
from sklearn.ensemble import RandomForestClassifier
from sklearn.naive_bayes import GaussianNB


METHOD = {"svm":svm.SVC, "rf":RandomForestClassifier, "logistic":LogisticRegression}

def model_generate(feature,lable,method):
    clf = RandomForestClassifier(n_estimators = 1000)
    clf.fit(feature,lable)
    return clf

def classify(clf, predict_feature,prob=False):
    if prob:
        result = clf.predict_proba(np.array(predict_feature))[0]
    else:
        result = clf.predict(np.array(predict_feature))[0]

    return result

def classify_test(feature,lable,method):
    clf = METHOD[method]()
    clf.fit(feature,lable)
    scores = cross_validation.cross_val_score(clf, np.array(feature), np.array(lable), cv=5)
    print scores

def judge(predict,target):
    t1=t2=t3=t4=0
    for i in range(len(target)):
        if target[i]==1:
            t1 += 1
            if predict[i]==1:
                t2 += 1
        else:
            t3 += 1
            if predict[i]==0:
                t4 += 1
                
    print "pp {}".format(float(t2)/t1)
    print "nn {}".format(float(t4)/t3)


def predict(test_feature):
    lable=[]
    raw = [-0.008423022929340239, 0.014111922141119237, 0.005421245421245436, -0.052051168945743416, -0.011389521640091117, 0.021952043228638987, 0.00960614793467816, -0.013936015511391162, -0.004324511330219685, -0.011297709923664117, 0.004144144144144152, 0.0015463917525773783, 0.00294117647058822, 0.011111111111111157, -0.010207336523125969, 0.005727376861397357, 0.003444316877152624, -0.0535818287711125, -0.003182686187141965, 0.03875000000000006, 0.0948509485094851, 0.019644723092998945, 0.02481774468745155, 0.0117884287265395, 0.04373576309794993, -0.03244704821991891, 0.005105105105105085, 0.0007356547327121416, -0.010471204188481823, 0.020000000000000007, 0.053453453453453495, -0.0005861664712777263, -0.0055309734513274145, 0.022557695644629543, 0.02279442802870406, -0.059368421052631584, 0.09999999999999994, 0.038618618618618615, -0.06975206611570246, 0.056583629893238424, -0.03775582215949189, 0.06834249803613505, 0.053846153846153814, -0.009318996415770539, 0.017188508715300185, 0.04432348367029552, 0.03216919932462459, 0.023521098324247914, 0.012813370473537629, -0.06502590673575125, 0.0, -0.06664550936210731, -0.012147239263803684, 0.016101979201610214, -0.020449212202480707, 0.007788689468337258, -0.006155143338954443, 0.020064724919093883]
    for i in raw:
        if i>0:
            lable.append(1)
        else:
            lable.append(0)

    feature= [[0.8, 0.4, 0.4, 0.4, 0.5], [0.6666666666666666, 0.5, 0.6, 0.6, 0.5], [0.6, 0.2, 0.4, 0.5, 0.5], [0.4, 0.4, 0.4, 0.6, 0.5], [0.6, 0.6, 0.4, 0.6, 0.6], [0.6, 0.6, 0.6, 0.8, 0.75], [0.6, 0.25, 0.5, 0.75, 0.6666666666666666], [0.4, 0.4, 0.25, 0.5, 0.5], [0.75, 0.75, 0.75, 0.75, 0.75], [0.5, 1.0, 1.0, 0.6666666666666666, 0.6666666666666666], [0.0, 1.0, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666], [0.25, 0.75, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333], [0.4, 0.8, 0.5, 0.5, 0.3333333333333333], [0.2, 0.6, 0.6, 0.8, 0.75], [0.2, 0.6, 0.6, 1.0, 0.75], [0.5, 0.5, 0.5, 1.0, 0.6666666666666666], [0.4, 0.6, 0.5, 1.0, 0.75], [0.3333333333333333, 0.6666666666666666, 0.4, 1.0, 0.75], [0.5, 0.8333333333333334, 0.6, 1.0, 1.0], [0.4, 0.75, 0.5, 0.75, 0.75], [1.0, 0.5, 0.5, 0.0, 0.0], [0.8, 0.5, 0.5, 0.0, 0.0], [0.5, 0.5, 0.3333333333333333, 0.3333333333333333, 0.0], [0.5, 0.75, 0.75, 0.75, 0.75], [0.6, 0.6, 0.6, 0.6, 0.75], [0.6, 0.25, 0.25, 0.25, 0.3333333333333333], [0.8, 0.5, 0.5, 0.5, 0.3333333333333333], [0.8, 0.5, 0.5, 0.5, 0.5], [1.0, 1.0, 1.0, 1.0, 1.0], [0.4, 0.4, 0.25, 0.5, 0.5], [0.2, 0.0, 0.0, 0.0, 0.0], [0.16666666666666666, 0.0, 0.0, 0.0, 0.0], [0.3333333333333333, 0.0, 0.16666666666666666, 0.0, 0.0], [0.4, 0.2, 0.4, 0.5, 0.5], [0.3333333333333333, 0.3333333333333333, 0.2, 0.4, 0.2], [0.3333333333333333, 0.16666666666666666, 0.0, 0.2, 0.0], [0.42857142857142855, 0.2857142857142857, 0.2857142857142857, 0.3333333333333333, 0.3333333333333333], [0.25, 0.14285714285714285, 0.2857142857142857, 0.16666666666666666, 0.3333333333333333], [0.5714285714285714, 0.3333333333333333, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], [0.5, 0.3333333333333333, 0.16666666666666666, 0.2, 0.2], [0.6, 0.4, 0.2, 0.4, 0.4], [0.8, 0.6, 0.4, 0.5, 0.5], [0.6, 0.6, 0.6, 0.5, 0.5], [0.3333333333333333, 0.5, 0.4, 0.6, 0.5], [0.3333333333333333, 0.3333333333333333, 0.5, 0.5, 0.6], [0.3333333333333333, 0.5, 0.4, 0.0, 0.0], [0.4, 0.4, 0.4, 0.0, 0.0], [1.0, 0.75, 0.6666666666666666, 0.6666666666666666, 1.0], [0.6666666666666666, 0.5, 0.4, 0.4, 0.6], [0.3333333333333333, 0.5, 0.4, 0.2, 0.2], [0.42857142857142855, 0.5714285714285714, 0.5714285714285714, 0.3333333333333333, 0.3333333333333333], [0.42857142857142855, 0.5, 0.6666666666666666, 0.4, 0.4], [0.5, 0.5, 0.8333333333333334, 0.8333333333333334, 0.8], [0.8, 0.4, 0.75, 0.75, 0.75], [0.75, 0.25, 0.75, 0.75, 0.75], [0.8333333333333334, 0.4, 0.6, 0.6, 0.75], [0.5, 0.6666666666666666, 0.4, 0.4, 0.5], [0.16666666666666666, 0.5, 0.4, 0.4, 0.6]]




    clf = model_generate(feature,lable,"rf")
    return classify(clf,test_feature)

